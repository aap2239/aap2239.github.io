{"status":"ok","feed":{"url":"https://medium.com/feed/@aishwaryapatange","title":"Stories by Aishwarya Patange on Medium","link":"https://medium.com/@aishwaryapatange?source=rss-412da8df2d90------2","author":"","description":"Stories by Aishwarya Patange on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*025_mYptIMnnOomiPLKSCA.jpeg"},"items":[{"title":"Low Latency Streaming Solutions","pubDate":"2023-02-07 12:30:52","link":"https://aishwaryapatange.medium.com/low-latency-streaming-solutions-8f73bd158ef1?source=rss-412da8df2d90------2","guid":"https://medium.com/p/8f73bd158ef1","author":"Aishwarya Patange","thumbnail":"","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*bSewKX8YKOBVgK8A-CleKg.jpeg\"></figure><h3>Introduction</h3>\n<p>In 2019, Zenith Media published a seminal report entitled \u201cOnline Video Forecasts,\u201d which presented insightful data on the rapidly growing trend of online video streaming. The report found that, on average, individuals spend 100 minutes each day consuming video content online, with certain countries, including China, Sweden, Canada, India, Mexico, the United Kingdom, and the United States, exhibiting even higher usage rates, surpassing the average with daily consumption exceeding 100 minutes by\u00a02021.</p>\n<p>The report emphasized the paramount importance of the video experience in determining an individual\u2019s level of engagement and interest in online video content. One key factor that significantly impacts the quality of the video experience is the reduction of latency in the video stream. Thus, minimizing latency is considered a crucial aspect in ensuring a seamless and satisfying video experience for\u00a0users.</p>\n<blockquote>Our data shows that <strong>just one buffering event decreases the amount of video watched by 39%</strong>.\u200a\u2014<a href=\"https://www.mux.com/blog/buffering-reduces-video-watch-time-by-40-according-to-research\">\u200aMux.com</a>\n</blockquote>\n<p>Allow me to define the term \u201clatency,\u201d which is central to the topic under consideration.<strong> Latency is the delay between when an image is captured on your camera and when your viewer experiences it on their\u00a0screen.</strong></p>\n<p>The comprehension of the principles behind low latency streaming holds significant value in terms of improving audience engagement. This is achieved through the utilization of Content Delivery Network (CDN) solutions, which have the capability to significantly reduce latency to mere milliseconds, thereby enhancing the overall viewing experience.</p>\n<h3>The Live Streaming Process</h3>\n<p>Streaming, a method of delivering media content in real-time over the internet, involves the sequential transmission of digital video data from a remote storage location to the client device. As opposed to downloading the entire video file beforehand, the streaming process enables the client device to receive and playback the video incrementally, resulting in an efficient utilization of bandwidth and improved user experience.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*gELQrChUFlU6V_9g-C6bzw.png\"><figcaption>Typical Video Streaming Pipeline</figcaption></figure><h3>Video Codecs</h3>\n<p>A Video Codec is a critical component of digital video technology, serving to both compress and decompress video data. In the late 1980s, various organizations embarked on exploratory efforts to leverage Discrete Cosine Transform-based lossy compression techniques for video encoding, which ultimately gave rise to the H.261 standard. Over the years, subsequent advancements in the field have resulted in the development of new standards, and the current industry standard is the Advanced Video Coding (AVC) standard, also known as H.264 and the High Efficiency Video Coding (HEVC) standard also called as H.265. Another important standard is the AV1 standard which is an open source, royalty free video coding format. With ongoing research and innovation, the Very Versatile Coding (VVC) standard, H.266, has been introduced and purports to offer a theoretical 50% improvement in efficiency compared to its predecessor. This improvement in bitrate efficiency comes at a cost of encoding decoding complexity.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/1*Fz07zDTTSUGhKYEXsb3Kaw.png\"><figcaption>Relative Bitrate Efficiency vs Codec (Lower the\u00a0Better)</figcaption></figure><p><em>This data was sourced from </em><a href=\"https://youtu.be/Frrw8O2ax6I?t=152\"><em>Jan Ozer</em></a><em>. The AV1 efficiency was approximated using the blog from a WinXDVD\u00a0</em><a href=\"https://www.winxdvd.com/convert-hevc-video/av1-vs-hevc.htm\"><em>blog</em></a><em>.</em></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/1*eGvKrQhrFYa0EjxwpHFFlw.png\"><figcaption>Encoding Time vs. Codec (Lower the\u00a0Better)</figcaption></figure><p><em>This information was procured from a </em><a href=\"https://www.winxdvd.com/video-transcoder/a-brief-intro-to-av1-codec.htm#vs\"><em>blog</em></a><em> hosted by WinXDVD. The VVC encoding time was approximated from another </em><a href=\"https://www.winxdvd.com/video-transcoder/h266-vvc-vs-av1.htm#efficiency\"><em>blog</em></a><em> by the same\u00a0company.</em></p>\n<p>Hence, it is evident that the selection of an appropriate video encoder is contingent upon the hardware specifications and whether the video is static or a live stream. In my opinion, optimal efficiency for live streaming video can be attained by utilizing either the AV1 codec or the Advanced Video Coding (AVC) codec, H.264, dependent upon the hardware used for encoding.</p>\n<h3>Media Streaming Protocols</h3>\n<p>Media streaming protocols represent a set of standardized procedures and guidelines that facilitate the fragmentation of video files into smaller, manageable segments for efficient delivery to end users. The files must undergo compression, achieved through the use of a codec. Furthermore, the files must be stored in a suitable container format, such as\u00a0.mp4 or\u00a0.avi, prior to transmission.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/1*uSK3vl8r3zUYcSVSWipBsQ.jpeg\"><figcaption>(sourced from a <a href=\"https://www.wowza.com/blog/2021-video-streaming-latency-report\">2021 Video Streaming Latency\u00a0Report</a>)</figcaption></figure><p>Here are a few important protocols:</p>\n<h4>Real Time Messaging Protocol</h4>\n<p>The Real-Time Messaging Protocol (RTMP) was originally developed by Adobe. RTMP operates as a TCP-based protocol (with the exception of RTMFP, a variation of RTMP that utilizes UDP) and facilitates persistent connections, enabling low-latency communication and minimizing buffering. It has evolved to serve a modified purpose in modern live streaming infrastructure. A drawback of RTMP is that as it does not use a regular internet protocol (like HTTP), and if you have users working behind a firewall, they probably can\u2019t get past it\u2019s restrictions and they will be blocked and unable to receive data. In contemporary live streaming environments, RTMP primarily serves as a means of delivering content from an encoder to an Online Video Platform, a process commonly referred to as ingestion.</p>\n<p>The utilization of RTMP for ingestion in live streaming setups offers a multitude of advantages. Firstly, it facilitates seamless integration with encoders due to its affordability and ease of deployment. Furthermore, the combination of RTMP and HLS results in a highly optimized low latency experience, maximizing the overall quality of the streaming experience.</p>\n<p>The RTMP ingest process sits between the Compression and Encoding block and the Online Video Platform block in the <a href=\"https://medium.com/p/8f73bd158ef1/#e7f9\">typical video streaming pipeline</a>. The implementation of RTMP ingestion necessitates the utilization of an RTMP encoder, which offers cost-effective and readily available options compared to other alternatives based on different streaming protocols.</p>\n<h4>Secure Reliable Transport</h4>\n<p>The Secure Reliable Transport (SRT) protocol is a solution designed by Haivision to address the limitations of the RTMP in delivering low-latency streams over challenging network conditions. SRT leverages the speed of the UDP and incorporates error correction mechanisms from the TCP to provide a reliable, low-latency streaming experience. Despite RTMP being widely used in the streaming industry, it is not optimized for interactivity, and SRT outperforms it by offering a significantly lower end-to-end latency of approximately 1 second compared to RTMP\u2019s 5 seconds. Furthermore, SRT is codec-agnostic, which means that it can support a wide range of video codecs without any restrictions. For these reasons, SRT has been replacing the RTMP ingestion protocol in many recent use-cases.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/1*hoQDvXQdPXcnRPGU2N8eBg.png\"><figcaption>Popular Ingestion Protocols (according to a <a href=\"https://www.wowza.com/blog/2021-video-streaming-latency-report\">2021 Video Streaming Latency\u00a0Report</a>)</figcaption></figure><h4>HTTP Live Streaming</h4>\n<p>HTTP Live Streaming (HLS) is an adaptive bitrate media streaming protocol that is designed to deliver visual and audio media to viewers over the internet. This protocol is developed by Apple and is characterized by its adaptive bitrate streaming nature, which determines the most optimal bitrate for a given client by first determining the bandwidth of the client. HLS was introduced as a successor to the Quicktime Streaming Server (QSS), which was utilized by early smartphones and was instrumental in the early days of digital media delivery. However, QSS relied on non-standard ports for data transfer, which resulted in firewall restrictions and limitations similar to\u00a0RTMP.</p>\n<p>It operates by dividing a video into segments, with each segment being a short clip encoded using the H.264 or H.265 codecs. These segments are stored on an HTTP server and are accompanied by a manifest file with a *.m3u8 extension, which serves as an index for the video\u00a0chunks.</p>\n<p>It boasts a high degree of compatibility with a vast array of devices and firewalls, making it a widely accessible and practical solution for delivering multimedia content. Additionally, the utilization of custom HLS encoders allows for a reduction in\u00a0latency.</p>\n<p>The limitations of the protocol can be attributed to its relatively elevated latency, which can result in delays of up to 30 seconds. Additionally, there is a requirement for a minimum of three video segments to be maintained in the queue prior to permitting video playback, which can be considered as another drawback of\u00a0HLS.</p>\n<p>For this reason, we use the RTMP ingest (mentioned above) with the HSL streaming protocol as it yield the lowest latency possible.</p>\n<h4>Dynamic Adaptive Streaming over\u00a0HTTP</h4>\n<p>Also referred to as MPEG-DASH, DASH is a vendor-independent, adaptive bitrate streaming protocol that provides a viable alternative to the HLS protocol. Similar to HLS, DASH operates by segmenting the video content into small HTTP-based segments and serving them through a dynamic XML-based manifest file that acts as an index to these segments. One of the major advantages of DASH is its codec-agnostic nature, enabling the delivery of video content encoded in any codec format. However, it is worth noting that DASH does not have as widespread support as HLS and may not be supported by all\u00a0devices.</p>\n<h4>Web Real Time Communication</h4>\n<p>WebRTC is a cutting-edge technology designed with low latency as a primary consideration. Developed by Google, it is not a protocol in the traditional sense, but rather an open-source framework that encompasses multiple standard protocols, streaming components, and a web API. Its unique advantage lies in its reliance on the UDP, which allows for a direct, real-time connection between source and endpoint without the need for a prior handshake. This direct connection also eliminates the need for content splicing or splitting, and the technology is equipped with various mechanisms to mitigate the impact of packet loss, garbled order and delayed\u00a0arrival.</p>\n<p>However, there are some trade-offs to consider, such as limitations in terms of scalability and quality. Additionally, the use of WebRTC is restricted to a limited range of video and audio codecs, with VP8/VP9 and H.264 with B frames being the only supported formats video codecs. The standard implementation of WebRTC also incorporates encryption, which increases the handshake times and the transmission bandwidth.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/721/1*XzMXimL_1HSbNfI38LbVew.png\"><figcaption>Popular Streaming Protocols (according to a <a href=\"https://www.wowza.com/blog/2021-video-streaming-latency-report\">2021 Video Streaming Latency\u00a0Report</a>)</figcaption></figure><h3>Traditional Low Latency Streaming Solution</h3>\n<p>Latency has become a prominent issue in the realm of live streaming, hindering the delivery of an optimal viewing experience to audiences. The demands of modern streaming audiences have evolved beyond what traditional broadcast methods can provide, necessitating the implementation of ultra-low latency solutions to effectively address this challenge.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1tjn-Z04qyURt7QcD44pxw.png\"><figcaption>Traditional Low Latency Streaming Solutions</figcaption></figure><p>The commonly adopted industry standard for live streaming is the HLS protocol, which involves an initial RTMP ingestion, followed by conversion to HLS by the video platform for distribution. While this approach offers a high-quality stream to viewers, it can introduce significant latencies, often exceeding 15 seconds. The use of a standard HLS ingest can further exacerbate this\u00a0latency.</p>\n<h3>Low Latency Streaming Solution Proposed by CDNetworks</h3>\n<p>CDNetworks has innovatively devised a customized version of the WebRTC technology, incorporating enhancements to the conventional CDN network architecture. This custom implementation overcomes the limitations of traditional WebRTC and achieves a remarkable latency of less than 500 milliseconds.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*UZFxl3DXuObu6e5fiOY7Ag.png\"><figcaption>CDNetwork\u2019s Low Latency Streaming Solution</figcaption></figure><p>To enhance the delivery of superior streams, CDNetworks has made several optimizations, including:</p>\n<ul>\n<li>Support for the advanced H.265 and AAC video and audio transcoding formats.</li>\n<li>Incorporation of B frame support for a seamless playback experience.</li>\n<li>The elimination of DTLS encryption from the standard WebRTC and the implementation of proprietary security strategies that have a lower transmission latency.</li>\n</ul>\n<p>In addition to these optimizations, CDNetworks also offers traditional livestream features such as watermarking, transcoding, and recording, making it a highly viable platform for live streaming.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8f73bd158ef1\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*bSewKX8YKOBVgK8A-CleKg.jpeg\"></figure><h3>Introduction</h3>\n<p>In 2019, Zenith Media published a seminal report entitled \u201cOnline Video Forecasts,\u201d which presented insightful data on the rapidly growing trend of online video streaming. The report found that, on average, individuals spend 100 minutes each day consuming video content online, with certain countries, including China, Sweden, Canada, India, Mexico, the United Kingdom, and the United States, exhibiting even higher usage rates, surpassing the average with daily consumption exceeding 100 minutes by\u00a02021.</p>\n<p>The report emphasized the paramount importance of the video experience in determining an individual\u2019s level of engagement and interest in online video content. One key factor that significantly impacts the quality of the video experience is the reduction of latency in the video stream. Thus, minimizing latency is considered a crucial aspect in ensuring a seamless and satisfying video experience for\u00a0users.</p>\n<blockquote>Our data shows that <strong>just one buffering event decreases the amount of video watched by 39%</strong>.\u200a\u2014<a href=\"https://www.mux.com/blog/buffering-reduces-video-watch-time-by-40-according-to-research\">\u200aMux.com</a>\n</blockquote>\n<p>Allow me to define the term \u201clatency,\u201d which is central to the topic under consideration.<strong> Latency is the delay between when an image is captured on your camera and when your viewer experiences it on their\u00a0screen.</strong></p>\n<p>The comprehension of the principles behind low latency streaming holds significant value in terms of improving audience engagement. This is achieved through the utilization of Content Delivery Network (CDN) solutions, which have the capability to significantly reduce latency to mere milliseconds, thereby enhancing the overall viewing experience.</p>\n<h3>The Live Streaming Process</h3>\n<p>Streaming, a method of delivering media content in real-time over the internet, involves the sequential transmission of digital video data from a remote storage location to the client device. As opposed to downloading the entire video file beforehand, the streaming process enables the client device to receive and playback the video incrementally, resulting in an efficient utilization of bandwidth and improved user experience.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*gELQrChUFlU6V_9g-C6bzw.png\"><figcaption>Typical Video Streaming Pipeline</figcaption></figure><h3>Video Codecs</h3>\n<p>A Video Codec is a critical component of digital video technology, serving to both compress and decompress video data. In the late 1980s, various organizations embarked on exploratory efforts to leverage Discrete Cosine Transform-based lossy compression techniques for video encoding, which ultimately gave rise to the H.261 standard. Over the years, subsequent advancements in the field have resulted in the development of new standards, and the current industry standard is the Advanced Video Coding (AVC) standard, also known as H.264 and the High Efficiency Video Coding (HEVC) standard also called as H.265. Another important standard is the AV1 standard which is an open source, royalty free video coding format. With ongoing research and innovation, the Very Versatile Coding (VVC) standard, H.266, has been introduced and purports to offer a theoretical 50% improvement in efficiency compared to its predecessor. This improvement in bitrate efficiency comes at a cost of encoding decoding complexity.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/1*Fz07zDTTSUGhKYEXsb3Kaw.png\"><figcaption>Relative Bitrate Efficiency vs Codec (Lower the\u00a0Better)</figcaption></figure><p><em>This data was sourced from </em><a href=\"https://youtu.be/Frrw8O2ax6I?t=152\"><em>Jan Ozer</em></a><em>. The AV1 efficiency was approximated using the blog from a WinXDVD\u00a0</em><a href=\"https://www.winxdvd.com/convert-hevc-video/av1-vs-hevc.htm\"><em>blog</em></a><em>.</em></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/1*eGvKrQhrFYa0EjxwpHFFlw.png\"><figcaption>Encoding Time vs. Codec (Lower the\u00a0Better)</figcaption></figure><p><em>This information was procured from a </em><a href=\"https://www.winxdvd.com/video-transcoder/a-brief-intro-to-av1-codec.htm#vs\"><em>blog</em></a><em> hosted by WinXDVD. The VVC encoding time was approximated from another </em><a href=\"https://www.winxdvd.com/video-transcoder/h266-vvc-vs-av1.htm#efficiency\"><em>blog</em></a><em> by the same\u00a0company.</em></p>\n<p>Hence, it is evident that the selection of an appropriate video encoder is contingent upon the hardware specifications and whether the video is static or a live stream. In my opinion, optimal efficiency for live streaming video can be attained by utilizing either the AV1 codec or the Advanced Video Coding (AVC) codec, H.264, dependent upon the hardware used for encoding.</p>\n<h3>Media Streaming Protocols</h3>\n<p>Media streaming protocols represent a set of standardized procedures and guidelines that facilitate the fragmentation of video files into smaller, manageable segments for efficient delivery to end users. The files must undergo compression, achieved through the use of a codec. Furthermore, the files must be stored in a suitable container format, such as\u00a0.mp4 or\u00a0.avi, prior to transmission.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/1*uSK3vl8r3zUYcSVSWipBsQ.jpeg\"><figcaption>(sourced from a <a href=\"https://www.wowza.com/blog/2021-video-streaming-latency-report\">2021 Video Streaming Latency\u00a0Report</a>)</figcaption></figure><p>Here are a few important protocols:</p>\n<h4>Real Time Messaging Protocol</h4>\n<p>The Real-Time Messaging Protocol (RTMP) was originally developed by Adobe. RTMP operates as a TCP-based protocol (with the exception of RTMFP, a variation of RTMP that utilizes UDP) and facilitates persistent connections, enabling low-latency communication and minimizing buffering. It has evolved to serve a modified purpose in modern live streaming infrastructure. A drawback of RTMP is that as it does not use a regular internet protocol (like HTTP), and if you have users working behind a firewall, they probably can\u2019t get past it\u2019s restrictions and they will be blocked and unable to receive data. In contemporary live streaming environments, RTMP primarily serves as a means of delivering content from an encoder to an Online Video Platform, a process commonly referred to as ingestion.</p>\n<p>The utilization of RTMP for ingestion in live streaming setups offers a multitude of advantages. Firstly, it facilitates seamless integration with encoders due to its affordability and ease of deployment. Furthermore, the combination of RTMP and HLS results in a highly optimized low latency experience, maximizing the overall quality of the streaming experience.</p>\n<p>The RTMP ingest process sits between the Compression and Encoding block and the Online Video Platform block in the <a href=\"https://medium.com/p/8f73bd158ef1/#e7f9\">typical video streaming pipeline</a>. The implementation of RTMP ingestion necessitates the utilization of an RTMP encoder, which offers cost-effective and readily available options compared to other alternatives based on different streaming protocols.</p>\n<h4>Secure Reliable Transport</h4>\n<p>The Secure Reliable Transport (SRT) protocol is a solution designed by Haivision to address the limitations of the RTMP in delivering low-latency streams over challenging network conditions. SRT leverages the speed of the UDP and incorporates error correction mechanisms from the TCP to provide a reliable, low-latency streaming experience. Despite RTMP being widely used in the streaming industry, it is not optimized for interactivity, and SRT outperforms it by offering a significantly lower end-to-end latency of approximately 1 second compared to RTMP\u2019s 5 seconds. Furthermore, SRT is codec-agnostic, which means that it can support a wide range of video codecs without any restrictions. For these reasons, SRT has been replacing the RTMP ingestion protocol in many recent use-cases.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/1*hoQDvXQdPXcnRPGU2N8eBg.png\"><figcaption>Popular Ingestion Protocols (according to a <a href=\"https://www.wowza.com/blog/2021-video-streaming-latency-report\">2021 Video Streaming Latency\u00a0Report</a>)</figcaption></figure><h4>HTTP Live Streaming</h4>\n<p>HTTP Live Streaming (HLS) is an adaptive bitrate media streaming protocol that is designed to deliver visual and audio media to viewers over the internet. This protocol is developed by Apple and is characterized by its adaptive bitrate streaming nature, which determines the most optimal bitrate for a given client by first determining the bandwidth of the client. HLS was introduced as a successor to the Quicktime Streaming Server (QSS), which was utilized by early smartphones and was instrumental in the early days of digital media delivery. However, QSS relied on non-standard ports for data transfer, which resulted in firewall restrictions and limitations similar to\u00a0RTMP.</p>\n<p>It operates by dividing a video into segments, with each segment being a short clip encoded using the H.264 or H.265 codecs. These segments are stored on an HTTP server and are accompanied by a manifest file with a *.m3u8 extension, which serves as an index for the video\u00a0chunks.</p>\n<p>It boasts a high degree of compatibility with a vast array of devices and firewalls, making it a widely accessible and practical solution for delivering multimedia content. Additionally, the utilization of custom HLS encoders allows for a reduction in\u00a0latency.</p>\n<p>The limitations of the protocol can be attributed to its relatively elevated latency, which can result in delays of up to 30 seconds. Additionally, there is a requirement for a minimum of three video segments to be maintained in the queue prior to permitting video playback, which can be considered as another drawback of\u00a0HLS.</p>\n<p>For this reason, we use the RTMP ingest (mentioned above) with the HSL streaming protocol as it yield the lowest latency possible.</p>\n<h4>Dynamic Adaptive Streaming over\u00a0HTTP</h4>\n<p>Also referred to as MPEG-DASH, DASH is a vendor-independent, adaptive bitrate streaming protocol that provides a viable alternative to the HLS protocol. Similar to HLS, DASH operates by segmenting the video content into small HTTP-based segments and serving them through a dynamic XML-based manifest file that acts as an index to these segments. One of the major advantages of DASH is its codec-agnostic nature, enabling the delivery of video content encoded in any codec format. However, it is worth noting that DASH does not have as widespread support as HLS and may not be supported by all\u00a0devices.</p>\n<h4>Web Real Time Communication</h4>\n<p>WebRTC is a cutting-edge technology designed with low latency as a primary consideration. Developed by Google, it is not a protocol in the traditional sense, but rather an open-source framework that encompasses multiple standard protocols, streaming components, and a web API. Its unique advantage lies in its reliance on the UDP, which allows for a direct, real-time connection between source and endpoint without the need for a prior handshake. This direct connection also eliminates the need for content splicing or splitting, and the technology is equipped with various mechanisms to mitigate the impact of packet loss, garbled order and delayed\u00a0arrival.</p>\n<p>However, there are some trade-offs to consider, such as limitations in terms of scalability and quality. Additionally, the use of WebRTC is restricted to a limited range of video and audio codecs, with VP8/VP9 and H.264 with B frames being the only supported formats video codecs. The standard implementation of WebRTC also incorporates encryption, which increases the handshake times and the transmission bandwidth.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/721/1*XzMXimL_1HSbNfI38LbVew.png\"><figcaption>Popular Streaming Protocols (according to a <a href=\"https://www.wowza.com/blog/2021-video-streaming-latency-report\">2021 Video Streaming Latency\u00a0Report</a>)</figcaption></figure><h3>Traditional Low Latency Streaming Solution</h3>\n<p>Latency has become a prominent issue in the realm of live streaming, hindering the delivery of an optimal viewing experience to audiences. The demands of modern streaming audiences have evolved beyond what traditional broadcast methods can provide, necessitating the implementation of ultra-low latency solutions to effectively address this challenge.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1tjn-Z04qyURt7QcD44pxw.png\"><figcaption>Traditional Low Latency Streaming Solutions</figcaption></figure><p>The commonly adopted industry standard for live streaming is the HLS protocol, which involves an initial RTMP ingestion, followed by conversion to HLS by the video platform for distribution. While this approach offers a high-quality stream to viewers, it can introduce significant latencies, often exceeding 15 seconds. The use of a standard HLS ingest can further exacerbate this\u00a0latency.</p>\n<h3>Low Latency Streaming Solution Proposed by CDNetworks</h3>\n<p>CDNetworks has innovatively devised a customized version of the WebRTC technology, incorporating enhancements to the conventional CDN network architecture. This custom implementation overcomes the limitations of traditional WebRTC and achieves a remarkable latency of less than 500 milliseconds.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*UZFxl3DXuObu6e5fiOY7Ag.png\"><figcaption>CDNetwork\u2019s Low Latency Streaming Solution</figcaption></figure><p>To enhance the delivery of superior streams, CDNetworks has made several optimizations, including:</p>\n<ul>\n<li>Support for the advanced H.265 and AAC video and audio transcoding formats.</li>\n<li>Incorporation of B frame support for a seamless playback experience.</li>\n<li>The elimination of DTLS encryption from the standard WebRTC and the implementation of proprietary security strategies that have a lower transmission latency.</li>\n</ul>\n<p>In addition to these optimizations, CDNetworks also offers traditional livestream features such as watermarking, transcoding, and recording, making it a highly viable platform for live streaming.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8f73bd158ef1\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["media-streaming","low-latency-streaming","video-streaming-service","video-coding"]},{"title":"PyEarth\u200a\u2014\u200aMultivariate Adaptive Regression Splines on Python","pubDate":"2022-04-26 11:31:57","link":"https://aishwaryapatange.medium.com/pyearth-multivariate-adaptive-regression-splines-on-python-86e8dd647030?source=rss-412da8df2d90------2","guid":"https://medium.com/p/86e8dd647030","author":"Aishwarya Patange","thumbnail":"","description":"\n<h3>PyEarth\u200a\u2014\u200aMultivariate Adaptive Regression Splines on\u00a0Python</h3>\n<p>Multivariate Adaptive Regression Splines (MARS) is a form of <strong>non-parametric</strong> regression analysis technique which automatically models <strong>non-linearities</strong> and <strong>interactions</strong> between features.</p>\n<p>It was developed by <a href=\"https://en.wikipedia.org/wiki/Jerome_H._Friedman\">Jerome H. Friedman</a> in\u00a0<a href=\"http://www.stat.yale.edu/~lc436/08Spring665/Mars_Friedman_91.pdf\">1991</a>.</p>\n<p>The term \u201cMARS\u201d is trademarked and licensed exclusively to <a href=\"https://www.salford-systems.com/\">Salford Systems</a>. We can use MARS as an abbreviation; however, it cannot be used for competing software solutions. This is why the Python package uses the name <strong>earth</strong>. Although, according to the package documentation, a backronym for \u201cearth\u201d is \u201cEnhanced Adaptive Regression Through\u00a0Hinges\u201d.</p>\n<p>Let\u2019s start from the\u00a0basics.</p>\n<h3>Limitations of Linear Regression</h3>\n<p>Although linear regression is one of the easiest regression techniques to implement, it does have a lot of drawbacks. Since linear regression assumes a linear relationship between the input and output variables, it fails to fit complex datasets properly. In most real-life scenarios the relationship between the variables of the dataset isn\u2019t linear and hence a straight line doesn\u2019t fit the data properly.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/1*RiUl412bQLeTftwCP2F6fw.png\"><figcaption>Underfitting due to Linear Regression</figcaption></figure><p>Moreover, data outliers can damage the performance of a linear model drastically and can often lead to models with low accuracy.</p>\n<p>These linear models can be adapted to nonlinear patterns in the data by manually adding nonlinear model terms (e.g., squared terms, interaction effects, and other transformations of the original features); however, to do so you, the analyst, must know the specific nature of the non-linearities and interactions <em>a priori</em>. This kind of regression is called parametric regression.</p>\n<p>Alternatively, there are numerous algorithms that are inherently non-linear. When using these models, the exact form of the non-linearity does not need to be known explicitly or specified prior to model training. Rather, these algorithms will search for, and discover, non-linearities and interactions in the data that help maximise predictive accuracy. However, many of these algorithms lack the simplicity which linear regression offers making them a bit difficult to interpret.</p>\n<p>A good balance of non-linearity and interpretebility is offered by the Piecewise Defined Functions family.</p>\n<h3>Piecewise Defined Functions</h3>\n<p>The basic idea behind piecewise linear regression is that if the data follows different linear trends over different regions, then we model the regression function in \u201cpieces.\u201d The pieces can be connected or not connected.</p>\n<p>Piecewise definition is actually a way of expressing the function, rather than a characteristic of the function itself. A distinct, but related notion is that of a property holding piecewise for a function, used when the domain can be <a href=\"https://en.wikipedia.org/wiki/Partition_of_an_interval\">divided into intervals</a> on which the property holds. Unlike the notion above, this is actually a property of the function\u00a0itself.</p>\n<p>Piecewise functions can be defined using the common <a href=\"https://en.wikipedia.org/wiki/Functional_notation\">functional notation</a>, where the body of the function is an array of functions and associated subdomains. These subdomains together must cover the whole <a href=\"https://en.wikipedia.org/wiki/Domain_of_a_function\">domain</a>; often it is also required that they are pairwise disjoint, i.e. form a partition of the\u00a0domain.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*dlVL5PxHt0D1jAwmT7G_wQ.png\"><figcaption>Piecewise Function</figcaption></figure><h3>The Hinge\u00a0Function</h3>\n<p>Instead of using the vanilla piecewise functions, the MARS algorithm uses another type of piecewise function called as the Hinge Function. It takes the form\u00a0of:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/106/1*84htI3A8xA5NymfkXliuHg.gif\"></figure><p>or</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/105/1*Rga1uyvFNx1MBgvCkVdm3Q.gif\"></figure><p>Where <em>c </em>is the knot point of the hinge function. The functions is 0 for a part of it\u2019s range and has a slope for the remainder, hence, it can be used to to partition the data into disjoint regions which can be treated independently. A mirror pair of hinge functions in the expression would\u00a0be:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/334/1*nNdWFvvOWS2i-tMDdyV2vQ.gif\"></figure><p>The main advantage of using this function over normal piecewise functions is that these functions can be multiplied together to form non-linear functions. Another advantage would be the fact that these help in creating continuous models with continuous derivatives and can also be used to model interactions.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/280/1*NUVQf1pVgBXVaEB3Wa05rQ.png\"><figcaption>Mirrored pair of hinge functions with a knot at\u00a0x=3.1</figcaption></figure><blockquote>This procedure is motivated by the recursive partitioning approach to regression and it\u2019s attractive properties. Unlike recursive partitioning, however, this method produces continuous models with continuous derivatives. It has more power and flexibility to model relationships that are nearly additive or involve interactions in at most a few variables.\u200a\u2014\u200aJerome H.\u00a0Friedman</blockquote>\n<h3>Multivariate Adaptive Regression Splines</h3>\n<p>MARS is a non-parametric regression procedure that makes no assumption about the underlying functional relationship between the dependent and independent variables. Instead, MARS constructs this relation from a set of coefficients and basis functions that are entirely \u201cdriven\u201d from the regression data. In a sense, the method is based on the \u201cdivide and conquer\u201d strategy, which partitions the input space into regions, each with its own regression equation. This makes MARS particularly suitable for problems with higher input dimensions (i.e., with more than 2 variables), where the curse of dimensionality would likely create problems for other techniques.</p>\n<h4>Forward Pass</h4>\n<p>The first part (called as the forward pass) is a kind of a recursive partitioning algorithm, however it can be viewed as a stepwise regression procedure. It is accomplished by the recursive splitting of the previous sub regions. The starting region is the entire domain D. The recursive subdivision is continued until a large number of sub regions are generated. Recursive partitioning is not only used to adjust the coefficient values to best fit the data, but also derive a good set of basis functions (sub regions) based on the data at hand. To overcome the limitations of recursive partitioning like functions being discontinuous at the sub region boundaries and having difficulties in approximating simple functions, we try to produce an equivalent model\u00a0to</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/260/1*XaqcyM8g37NF6q7SRtuaFQ.png\"><figcaption>Recursive Partition Regression Model</figcaption></figure><p>where Rm is the disjoint sub regions representing the partition of domain D and the functions gm are some simple parametric form functions by replacing the geometric concepts of regions and splitting of with the arithmetic notions of adding and multiplying. The recursive partitioning algorithm makes a basis function in the\u00a0form</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/381/1*LVw9xnEv2aJLrRykS5x9sw.png\"></figure><p>The quantity K\u2098 is the number of splits that gave rise to B\u2098, whereas the argument of the step functions (H) contains the parameters associated with each of these splits. q here refers to the order of the polynomial used. s\u2096\u2098 takes the value of +1 or -1 to indicate the direction of the step function. The v(k, m) label the predictor variables and the t\u2096\u2098 represent the values of the corresponding variables.</p>\n<h4>Backward Pass</h4>\n<p>Whenever we use forward stepwise regression algorithms, we almost always follow them up with a complimentary backward stepwise algorithm to remove the basis functions that no longer contribute sufficiently to the accuracy of the\u00a0fit.</p>\n<p>In fact, in recursive partitioning, the strategy used is to deliberately overfit the data with an excessively large model and then to trim it back until an optimal set is reached, based on a criterion which penalizes both for lack of fit and the increasing number of\u00a0regions.</p>\n<p>After applying the forward pass and the backward pass, we get a model in the form\u00a0of</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/478/1*Kq4WYKT8d-kQ1Nx_6m9sbw.png\"></figure><p>where a\u2080 is the coefficient of the constant basis function and the sum over the basis function showed\u00a0earlier.</p>\n<h4>Types of Basis Functions formed</h4>\n<p>There are three main kinds of basis functions we find after the forward and the backward pass has been completed</p>\n<ol><li>Constant Basis Functions (usually the intercept)</li></ol>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*eHznD7Az-aL2Zx7mMaJXnA.png\"></figure><p>2. Linear Basis Functions</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*YToyDQbTS_ZkM69LOVpAkw.png\"></figure><p>3. Hinge Functions</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*6ps2QYSrJrPiS28CACsmhQ.png\"></figure><h4>Interpreting the Model (in a\u00a0way)</h4>\n<p>By simply rearranging the terms, we can cast the model into a form that reveals considerable information about the predictive relation between the target and the input data. The idea is the collect together all the basis functions that involve identical predictor variable sets. Thus it can be recast into the\u00a0form:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/420/1*ZPF5FWfITOWRFTVB5Wse3w.png\"></figure><p>We won\u2019t delve too deep into the math behind the forward and the backward passes which this algorithm uses to get these knot\u00a0points.</p>\n<h4>Example of MARS fitting a Single Variable Prediction Dataset.</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1Wi7aiS2WlBA0L-ryMNo-w.png\"><figcaption>Prediction Using One\u00a0Variable</figcaption></figure><figure><img alt=\"Splines fitting the data with a knot around 1100 units on the x-axis\" src=\"https://cdn-images-1.medium.com/max/1024/1*29FapBHppoeaDxRYSk96pA.png\"><figcaption>Splines fitting the data with a knot around 1100 units on the\u00a0x-axis</figcaption></figure><h3>pyearth\u200a\u2014\u200aThe python API for\u00a0MARS</h3>\n<p>Python has it\u2019s own flavor of the MARS algorithm encapsulated in the pyearth package of scikit-learn-contrib. As mentioned earlier, \u201cearth\u201d is a backronym for \u201cEnhanced Adaptive Regression Through\u00a0Hinges\u201d.</p>\n<p>To install pyearth, a simple pip install won\u2019t work. Refer to this <a href=\"https://github.com/scikit-learn-contrib/py-earth/issues/210\">issue</a> for more details. Hence, to install this package, we will have to pip install a specific branch (<a href=\"https://github.com/scikit-learn-contrib/py-earth/tree/v0.2dev\">v0.2dev</a>) of the package from github. The command to install that specific package\u00a0is:</p>\n<pre>pip install git+https://github.com/scikit-learn-contrib/py-earth@v0.2dev</pre>\n<p><em>The repository is not actively maintained as of now and the last commit to the branch was made in 2019, hence there is a very low chance of the fix getting pushed to\u00a0master.</em></p>\n<h4>Using the\u00a0API</h4>\n<p>Like any other scikit-learn compatible estimator, pyearth has a fit-predict interface. It can also be integrated with the scikit-learn pipeline object to create complex\u00a0models.</p>\n<h4>Regression using\u00a0Earth</h4>\n<p>The Earth model is technically a pure regressor and can be used as one right out of the\u00a0box.</p>\n<p>You import the estimator using:</p>\n<pre>from pyearth import Earth</pre>\n<p>We will use a already available sklearn dataset for this example. We will split the dataset into training and testing split with a 0.3 test\u00a0size.</p>\n<pre>from sklearn.datasets import fetch_california_housing<br>from sklearn.model_selection import train_test_split<br><br>SEED = 42<br>data = fetch_california_housing()<br><br>X = data.data<br>y = data.target<br>column_names = data.feature_names<br><br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)</pre>\n<p>We will now fit the training data to the model. Here we are using a 2\u207f\u1d48 degree polynomial. This also helps you find out the feature interactions.</p>\n<pre>er = Earth(max_degree=2)<br><br>er.fit(X_train, y_train)</pre>\n<p>Now to find out the how well the model performed, we will use the mean squared error. The lower the metric is, the better the model has performed.</p>\n<pre>from sklearn.metrics import mean_squared_error<br><br>print(mean_squared_error(y_test, er.predict(X_test)))</pre>\n<p>The mean squared error obtained was: <strong>0.3670403639588185. </strong>If we compare this to a LinearRegression model, the LR model gives us a mean squared error of <strong>0.5305677824766755. </strong>As you can see, the Earth model gives us a much better mean squared\u00a0error.</p>\n<h4>Classification using\u00a0Earth</h4>\n<p>The Earth does not come with an inbuilt classifier as such. We link the output of the Earth regressor to a link function (like sigmoid, softmax, etc). In sklearn we can achieve this by coupling the Earth estimator with a LogisticRegression estimator. We first import the required libraries:</p>\n<pre>from pyearth import Earth<br>from sklearn.pipeline import Pipeline<br>from sklearn.linear_model import LogisticRegression</pre>\n<p>We will use another already available sklearn dataset for this example. We will split the dataset into training and testing split with a 0.3 test\u00a0size.</p>\n<pre>from sklearn.datasets import load_breast_cancer<br>from sklearn.model_selection import train_test_split</pre>\n<pre>SEED = 42<br>data = load_breast_cancer()</pre>\n<pre>X = data.data<br>y = data.target<br>column_names = data.feature_names</pre>\n<pre>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)</pre>\n<p>Now we will create a pipeline object which links Earth (with max_degree set to 1) to a LogisticRegression estimator and also fit the training set to\u00a0it.</p>\n<pre>ec = Pipeline(<br>    [<br>        (<br>            \"earth\",<br>            Earth(<br>                max_degree=1,<br>                penalty=1.5,<br>            ),<br>        ),<br>        (<br>            \"logistic\",<br>            LogisticRegression(),<br>        ),<br>    ]<br>)</pre>\n<pre>ec.fit(X_train, y_train)</pre>\n<p>The metric of our choice for classification would be accuracy score. We can evaluate the model by getting the accuracy of the\u00a0model.</p>\n<pre>from sklearn.metrics import accuracy_score</pre>\n<pre>print(accuracy_score(y_test, ec.predict(X_test)))</pre>\n<p>The accuracy score we got was:<strong> 0.9766081871345029</strong>. Comparing this to a vanilla LogisticRegression model, where we get an accuracy score of:<strong> 0.9707602339181286, </strong>we find out that the Earth Classifier is marginally better than the logistic regression model. But in datasets which have good local relations between X and y, this model will outperform LogisticRegression for\u00a0sure.</p>\n<h4><strong>Getting the Equations of the Basis Functions</strong></h4>\n<p>Getting the equations of the basis functions might reveal a lot about the relationship of the features and the response as well as give us an interpretation as to why the model made a specific prediction.</p>\n<p>To get the equations we can\u00a0use:</p>\n<pre>from pyearth import export<br>from sympy import Add</pre>\n<pre>for symbol in list(export.export_sympy(er).free_symbols):<br>    print(Add(*[argi for argi in export.export_sympy(er).args if argi.has(symbol)]))</pre>\n<p>Given below are a few equations I\u00a0got.</p>\n<ol>\n<li>0.4021923133082891</li>\n<li>-0.375263344751511*x6</li>\n<li>0.22492934193614*Max(0, 5.2606490872211\u200a\u2014\u200ax2) + 0.0783444118413456*Max(0, x2\u20135.2606490872211)</li>\n<li>0.0130407656826073*x1</li>\n<li>-0.520084136300528*Max(0, -x7\u2013123.73)\u200a\u2014\u200a0.374674796610147*Max(0, x7 +\u00a0123.73)</li>\n<li>-7.75060972107374e-5*Max(0, 2731.0\u200a\u2014\u200ax4)<br>-2.04395206229742*Max(0, 1.68276856524874\u200a\u2014\u200ax5) + 0.918559224621341*Max(0, 2.89189189189189\u200a\u2014\u200ax5) + 0.642218349447245*Max(0, x5\u20133.30545454545455)\u200a\u2014\u200a1.12051408620824*Max(0, x5\u20132.89189189189189) + 0.476248750006369*Max(0, x5\u20132.48760330578512)</li>\n<li>-1.59968468486322*Max(0, 1.35622317596567\u200a\u2014\u200ax3) + 0.735455706654561*Max(0, x3\u20131.35622317596567)\u200a\u2014\u200a1.03424762559189*Max(0, x3\u20131.09356725146199)</li>\n<li>0.413127792646243*Max(0, 1.8958\u200a\u2014\u200ax0)\u200a\u2014\u200a0.525288533388442*Max(0, 9.8074\u200a\u2014\u200ax0)\u200a\u2014\u200a0.0523808049123695*Max(0, x0\u20131.8958)</li>\n</ol>\n<p>As you can see, all of these are either one of the three basis functions (or a combination of them) mentioned above.</p>\n<h3>Conclusion</h3>\n<p>Too bored to write a conclusion right now. So I\u2019d like to conclude by saying it is a cool algorithm and y\u2019all should use it more\u00a0often.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=86e8dd647030\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>PyEarth\u200a\u2014\u200aMultivariate Adaptive Regression Splines on\u00a0Python</h3>\n<p>Multivariate Adaptive Regression Splines (MARS) is a form of <strong>non-parametric</strong> regression analysis technique which automatically models <strong>non-linearities</strong> and <strong>interactions</strong> between features.</p>\n<p>It was developed by <a href=\"https://en.wikipedia.org/wiki/Jerome_H._Friedman\">Jerome H. Friedman</a> in\u00a0<a href=\"http://www.stat.yale.edu/~lc436/08Spring665/Mars_Friedman_91.pdf\">1991</a>.</p>\n<p>The term \u201cMARS\u201d is trademarked and licensed exclusively to <a href=\"https://www.salford-systems.com/\">Salford Systems</a>. We can use MARS as an abbreviation; however, it cannot be used for competing software solutions. This is why the Python package uses the name <strong>earth</strong>. Although, according to the package documentation, a backronym for \u201cearth\u201d is \u201cEnhanced Adaptive Regression Through\u00a0Hinges\u201d.</p>\n<p>Let\u2019s start from the\u00a0basics.</p>\n<h3>Limitations of Linear Regression</h3>\n<p>Although linear regression is one of the easiest regression techniques to implement, it does have a lot of drawbacks. Since linear regression assumes a linear relationship between the input and output variables, it fails to fit complex datasets properly. In most real-life scenarios the relationship between the variables of the dataset isn\u2019t linear and hence a straight line doesn\u2019t fit the data properly.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/700/1*RiUl412bQLeTftwCP2F6fw.png\"><figcaption>Underfitting due to Linear Regression</figcaption></figure><p>Moreover, data outliers can damage the performance of a linear model drastically and can often lead to models with low accuracy.</p>\n<p>These linear models can be adapted to nonlinear patterns in the data by manually adding nonlinear model terms (e.g., squared terms, interaction effects, and other transformations of the original features); however, to do so you, the analyst, must know the specific nature of the non-linearities and interactions <em>a priori</em>. This kind of regression is called parametric regression.</p>\n<p>Alternatively, there are numerous algorithms that are inherently non-linear. When using these models, the exact form of the non-linearity does not need to be known explicitly or specified prior to model training. Rather, these algorithms will search for, and discover, non-linearities and interactions in the data that help maximise predictive accuracy. However, many of these algorithms lack the simplicity which linear regression offers making them a bit difficult to interpret.</p>\n<p>A good balance of non-linearity and interpretebility is offered by the Piecewise Defined Functions family.</p>\n<h3>Piecewise Defined Functions</h3>\n<p>The basic idea behind piecewise linear regression is that if the data follows different linear trends over different regions, then we model the regression function in \u201cpieces.\u201d The pieces can be connected or not connected.</p>\n<p>Piecewise definition is actually a way of expressing the function, rather than a characteristic of the function itself. A distinct, but related notion is that of a property holding piecewise for a function, used when the domain can be <a href=\"https://en.wikipedia.org/wiki/Partition_of_an_interval\">divided into intervals</a> on which the property holds. Unlike the notion above, this is actually a property of the function\u00a0itself.</p>\n<p>Piecewise functions can be defined using the common <a href=\"https://en.wikipedia.org/wiki/Functional_notation\">functional notation</a>, where the body of the function is an array of functions and associated subdomains. These subdomains together must cover the whole <a href=\"https://en.wikipedia.org/wiki/Domain_of_a_function\">domain</a>; often it is also required that they are pairwise disjoint, i.e. form a partition of the\u00a0domain.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*dlVL5PxHt0D1jAwmT7G_wQ.png\"><figcaption>Piecewise Function</figcaption></figure><h3>The Hinge\u00a0Function</h3>\n<p>Instead of using the vanilla piecewise functions, the MARS algorithm uses another type of piecewise function called as the Hinge Function. It takes the form\u00a0of:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/106/1*84htI3A8xA5NymfkXliuHg.gif\"></figure><p>or</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/105/1*Rga1uyvFNx1MBgvCkVdm3Q.gif\"></figure><p>Where <em>c </em>is the knot point of the hinge function. The functions is 0 for a part of it\u2019s range and has a slope for the remainder, hence, it can be used to to partition the data into disjoint regions which can be treated independently. A mirror pair of hinge functions in the expression would\u00a0be:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/334/1*nNdWFvvOWS2i-tMDdyV2vQ.gif\"></figure><p>The main advantage of using this function over normal piecewise functions is that these functions can be multiplied together to form non-linear functions. Another advantage would be the fact that these help in creating continuous models with continuous derivatives and can also be used to model interactions.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/280/1*NUVQf1pVgBXVaEB3Wa05rQ.png\"><figcaption>Mirrored pair of hinge functions with a knot at\u00a0x=3.1</figcaption></figure><blockquote>This procedure is motivated by the recursive partitioning approach to regression and it\u2019s attractive properties. Unlike recursive partitioning, however, this method produces continuous models with continuous derivatives. It has more power and flexibility to model relationships that are nearly additive or involve interactions in at most a few variables.\u200a\u2014\u200aJerome H.\u00a0Friedman</blockquote>\n<h3>Multivariate Adaptive Regression Splines</h3>\n<p>MARS is a non-parametric regression procedure that makes no assumption about the underlying functional relationship between the dependent and independent variables. Instead, MARS constructs this relation from a set of coefficients and basis functions that are entirely \u201cdriven\u201d from the regression data. In a sense, the method is based on the \u201cdivide and conquer\u201d strategy, which partitions the input space into regions, each with its own regression equation. This makes MARS particularly suitable for problems with higher input dimensions (i.e., with more than 2 variables), where the curse of dimensionality would likely create problems for other techniques.</p>\n<h4>Forward Pass</h4>\n<p>The first part (called as the forward pass) is a kind of a recursive partitioning algorithm, however it can be viewed as a stepwise regression procedure. It is accomplished by the recursive splitting of the previous sub regions. The starting region is the entire domain D. The recursive subdivision is continued until a large number of sub regions are generated. Recursive partitioning is not only used to adjust the coefficient values to best fit the data, but also derive a good set of basis functions (sub regions) based on the data at hand. To overcome the limitations of recursive partitioning like functions being discontinuous at the sub region boundaries and having difficulties in approximating simple functions, we try to produce an equivalent model\u00a0to</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/260/1*XaqcyM8g37NF6q7SRtuaFQ.png\"><figcaption>Recursive Partition Regression Model</figcaption></figure><p>where Rm is the disjoint sub regions representing the partition of domain D and the functions gm are some simple parametric form functions by replacing the geometric concepts of regions and splitting of with the arithmetic notions of adding and multiplying. The recursive partitioning algorithm makes a basis function in the\u00a0form</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/381/1*LVw9xnEv2aJLrRykS5x9sw.png\"></figure><p>The quantity K\u2098 is the number of splits that gave rise to B\u2098, whereas the argument of the step functions (H) contains the parameters associated with each of these splits. q here refers to the order of the polynomial used. s\u2096\u2098 takes the value of +1 or -1 to indicate the direction of the step function. The v(k, m) label the predictor variables and the t\u2096\u2098 represent the values of the corresponding variables.</p>\n<h4>Backward Pass</h4>\n<p>Whenever we use forward stepwise regression algorithms, we almost always follow them up with a complimentary backward stepwise algorithm to remove the basis functions that no longer contribute sufficiently to the accuracy of the\u00a0fit.</p>\n<p>In fact, in recursive partitioning, the strategy used is to deliberately overfit the data with an excessively large model and then to trim it back until an optimal set is reached, based on a criterion which penalizes both for lack of fit and the increasing number of\u00a0regions.</p>\n<p>After applying the forward pass and the backward pass, we get a model in the form\u00a0of</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/478/1*Kq4WYKT8d-kQ1Nx_6m9sbw.png\"></figure><p>where a\u2080 is the coefficient of the constant basis function and the sum over the basis function showed\u00a0earlier.</p>\n<h4>Types of Basis Functions formed</h4>\n<p>There are three main kinds of basis functions we find after the forward and the backward pass has been completed</p>\n<ol><li>Constant Basis Functions (usually the intercept)</li></ol>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*eHznD7Az-aL2Zx7mMaJXnA.png\"></figure><p>2. Linear Basis Functions</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*YToyDQbTS_ZkM69LOVpAkw.png\"></figure><p>3. Hinge Functions</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*6ps2QYSrJrPiS28CACsmhQ.png\"></figure><h4>Interpreting the Model (in a\u00a0way)</h4>\n<p>By simply rearranging the terms, we can cast the model into a form that reveals considerable information about the predictive relation between the target and the input data. The idea is the collect together all the basis functions that involve identical predictor variable sets. Thus it can be recast into the\u00a0form:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/420/1*ZPF5FWfITOWRFTVB5Wse3w.png\"></figure><p>We won\u2019t delve too deep into the math behind the forward and the backward passes which this algorithm uses to get these knot\u00a0points.</p>\n<h4>Example of MARS fitting a Single Variable Prediction Dataset.</h4>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*1Wi7aiS2WlBA0L-ryMNo-w.png\"><figcaption>Prediction Using One\u00a0Variable</figcaption></figure><figure><img alt=\"Splines fitting the data with a knot around 1100 units on the x-axis\" src=\"https://cdn-images-1.medium.com/max/1024/1*29FapBHppoeaDxRYSk96pA.png\"><figcaption>Splines fitting the data with a knot around 1100 units on the\u00a0x-axis</figcaption></figure><h3>pyearth\u200a\u2014\u200aThe python API for\u00a0MARS</h3>\n<p>Python has it\u2019s own flavor of the MARS algorithm encapsulated in the pyearth package of scikit-learn-contrib. As mentioned earlier, \u201cearth\u201d is a backronym for \u201cEnhanced Adaptive Regression Through\u00a0Hinges\u201d.</p>\n<p>To install pyearth, a simple pip install won\u2019t work. Refer to this <a href=\"https://github.com/scikit-learn-contrib/py-earth/issues/210\">issue</a> for more details. Hence, to install this package, we will have to pip install a specific branch (<a href=\"https://github.com/scikit-learn-contrib/py-earth/tree/v0.2dev\">v0.2dev</a>) of the package from github. The command to install that specific package\u00a0is:</p>\n<pre>pip install git+https://github.com/scikit-learn-contrib/py-earth@v0.2dev</pre>\n<p><em>The repository is not actively maintained as of now and the last commit to the branch was made in 2019, hence there is a very low chance of the fix getting pushed to\u00a0master.</em></p>\n<h4>Using the\u00a0API</h4>\n<p>Like any other scikit-learn compatible estimator, pyearth has a fit-predict interface. It can also be integrated with the scikit-learn pipeline object to create complex\u00a0models.</p>\n<h4>Regression using\u00a0Earth</h4>\n<p>The Earth model is technically a pure regressor and can be used as one right out of the\u00a0box.</p>\n<p>You import the estimator using:</p>\n<pre>from pyearth import Earth</pre>\n<p>We will use a already available sklearn dataset for this example. We will split the dataset into training and testing split with a 0.3 test\u00a0size.</p>\n<pre>from sklearn.datasets import fetch_california_housing<br>from sklearn.model_selection import train_test_split<br><br>SEED = 42<br>data = fetch_california_housing()<br><br>X = data.data<br>y = data.target<br>column_names = data.feature_names<br><br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)</pre>\n<p>We will now fit the training data to the model. Here we are using a 2\u207f\u1d48 degree polynomial. This also helps you find out the feature interactions.</p>\n<pre>er = Earth(max_degree=2)<br><br>er.fit(X_train, y_train)</pre>\n<p>Now to find out the how well the model performed, we will use the mean squared error. The lower the metric is, the better the model has performed.</p>\n<pre>from sklearn.metrics import mean_squared_error<br><br>print(mean_squared_error(y_test, er.predict(X_test)))</pre>\n<p>The mean squared error obtained was: <strong>0.3670403639588185. </strong>If we compare this to a LinearRegression model, the LR model gives us a mean squared error of <strong>0.5305677824766755. </strong>As you can see, the Earth model gives us a much better mean squared\u00a0error.</p>\n<h4>Classification using\u00a0Earth</h4>\n<p>The Earth does not come with an inbuilt classifier as such. We link the output of the Earth regressor to a link function (like sigmoid, softmax, etc). In sklearn we can achieve this by coupling the Earth estimator with a LogisticRegression estimator. We first import the required libraries:</p>\n<pre>from pyearth import Earth<br>from sklearn.pipeline import Pipeline<br>from sklearn.linear_model import LogisticRegression</pre>\n<p>We will use another already available sklearn dataset for this example. We will split the dataset into training and testing split with a 0.3 test\u00a0size.</p>\n<pre>from sklearn.datasets import load_breast_cancer<br>from sklearn.model_selection import train_test_split</pre>\n<pre>SEED = 42<br>data = load_breast_cancer()</pre>\n<pre>X = data.data<br>y = data.target<br>column_names = data.feature_names</pre>\n<pre>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)</pre>\n<p>Now we will create a pipeline object which links Earth (with max_degree set to 1) to a LogisticRegression estimator and also fit the training set to\u00a0it.</p>\n<pre>ec = Pipeline(<br>    [<br>        (<br>            \"earth\",<br>            Earth(<br>                max_degree=1,<br>                penalty=1.5,<br>            ),<br>        ),<br>        (<br>            \"logistic\",<br>            LogisticRegression(),<br>        ),<br>    ]<br>)</pre>\n<pre>ec.fit(X_train, y_train)</pre>\n<p>The metric of our choice for classification would be accuracy score. We can evaluate the model by getting the accuracy of the\u00a0model.</p>\n<pre>from sklearn.metrics import accuracy_score</pre>\n<pre>print(accuracy_score(y_test, ec.predict(X_test)))</pre>\n<p>The accuracy score we got was:<strong> 0.9766081871345029</strong>. Comparing this to a vanilla LogisticRegression model, where we get an accuracy score of:<strong> 0.9707602339181286, </strong>we find out that the Earth Classifier is marginally better than the logistic regression model. But in datasets which have good local relations between X and y, this model will outperform LogisticRegression for\u00a0sure.</p>\n<h4><strong>Getting the Equations of the Basis Functions</strong></h4>\n<p>Getting the equations of the basis functions might reveal a lot about the relationship of the features and the response as well as give us an interpretation as to why the model made a specific prediction.</p>\n<p>To get the equations we can\u00a0use:</p>\n<pre>from pyearth import export<br>from sympy import Add</pre>\n<pre>for symbol in list(export.export_sympy(er).free_symbols):<br>    print(Add(*[argi for argi in export.export_sympy(er).args if argi.has(symbol)]))</pre>\n<p>Given below are a few equations I\u00a0got.</p>\n<ol>\n<li>0.4021923133082891</li>\n<li>-0.375263344751511*x6</li>\n<li>0.22492934193614*Max(0, 5.2606490872211\u200a\u2014\u200ax2) + 0.0783444118413456*Max(0, x2\u20135.2606490872211)</li>\n<li>0.0130407656826073*x1</li>\n<li>-0.520084136300528*Max(0, -x7\u2013123.73)\u200a\u2014\u200a0.374674796610147*Max(0, x7 +\u00a0123.73)</li>\n<li>-7.75060972107374e-5*Max(0, 2731.0\u200a\u2014\u200ax4)<br>-2.04395206229742*Max(0, 1.68276856524874\u200a\u2014\u200ax5) + 0.918559224621341*Max(0, 2.89189189189189\u200a\u2014\u200ax5) + 0.642218349447245*Max(0, x5\u20133.30545454545455)\u200a\u2014\u200a1.12051408620824*Max(0, x5\u20132.89189189189189) + 0.476248750006369*Max(0, x5\u20132.48760330578512)</li>\n<li>-1.59968468486322*Max(0, 1.35622317596567\u200a\u2014\u200ax3) + 0.735455706654561*Max(0, x3\u20131.35622317596567)\u200a\u2014\u200a1.03424762559189*Max(0, x3\u20131.09356725146199)</li>\n<li>0.413127792646243*Max(0, 1.8958\u200a\u2014\u200ax0)\u200a\u2014\u200a0.525288533388442*Max(0, 9.8074\u200a\u2014\u200ax0)\u200a\u2014\u200a0.0523808049123695*Max(0, x0\u20131.8958)</li>\n</ol>\n<p>As you can see, all of these are either one of the three basis functions (or a combination of them) mentioned above.</p>\n<h3>Conclusion</h3>\n<p>Too bored to write a conclusion right now. So I\u2019d like to conclude by saying it is a cool algorithm and y\u2019all should use it more\u00a0often.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=86e8dd647030\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["regression","data-science","machine-learning","piecewise","spline-regression"]}]}